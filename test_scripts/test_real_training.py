import os
import yaml
import numpy as np
from skimage import io
import torch
import torch.nn as nn
import traceback
import argparse
import time

import datasets
import models
import utils
from datasets.queue import dequeue_and_enqueue
from datasets.degrade import SRMDPreprocessing

def create_config_for_real_data(config_path, liver_data_path, sdf_data_path, test_params):
    """åˆ›å»ºåŸºäºçœŸå®æ•°æ®çš„é…ç½®æ–‡ä»¶"""
    
    if not os.path.exists(os.path.dirname(config_path)):
        os.makedirs(os.path.dirname(config_path))

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(liver_data_path):
        raise FileNotFoundError(f"Liveræ•°æ®ç›®å½•ä¸å­˜åœ¨: {liver_data_path}")
    
    if not os.path.exists(sdf_data_path):
        raise FileNotFoundError(f"SDFæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {sdf_data_path}")
    
    # è®¡ç®—SDFç»Ÿè®¡ä¿¡æ¯
    print(f"æ­£åœ¨åŠ è½½SDFæ•°æ®: {sdf_data_path}")
    sdf_data = np.load(sdf_data_path).astype(np.float64)
    sdf_mean = np.mean(sdf_data)
    sdf_std = np.std(sdf_data)
    
    print(f"SDFæ•°æ®å½¢çŠ¶: {sdf_data.shape}")
    print(f"SDFå‡å€¼: {sdf_mean}")
    print(f"SDFæ ‡å‡†å·®: {sdf_std}")

    # è·å–ç»å¯¹è·¯å¾„
    liver_data_abs_path = os.path.abspath(liver_data_path)
    sdf_data_abs_path = os.path.abspath(sdf_data_path)

    # åˆ›å»ºé…ç½®æ–‡ä»¶å†…å®¹
    config_content = f"""
seed: 42
lambda_geom: {test_params['lambda_geom']}
batch_size: {test_params['batch_size']}
total_batch_size: {test_params['batch_size']}
sample_q: {test_params['sample_q']}
inp_size: {test_params['inp_size']}
queue_size: {test_params['batch_size'] * 2}

train_dataset1:
  dataset: {{name: image-volume, args: {{root_path: {liver_data_abs_path}, sdf_path: {sdf_data_abs_path}, repeat: 1}}}}
  wrapper: {{name: sr-gaussian, args: {{inp_size: {test_params['inp_size']}, sample_q: {test_params['sample_q']}, augment: false, scale: {test_params['scale']}}}}}

val_dataset1:
  dataset: {{name: image-volume, args: {{root_path: {liver_data_abs_path}, sdf_path: {sdf_data_abs_path}}}}}
  wrapper: {{name: sr-gaussian, args: {{inp_size: {test_params['inp_size']}, sample_q: {test_params['sample_q']}, scale: {test_params['scale']}}}}}

data_norm: {{inp: {{sub: [0.5], div: [0.5]}}, gt: {{sub: [0.5], div: [0.5]}}}}
sdf_norm: {{sub: [{sdf_mean}], div: [{sdf_std if sdf_std > 1e-6 else 1.0}]}}
optimizer: {{name: adam, args: {{lr: 1.e-4}}}}
epoch_max: 1

model:
  name: models
  args: {{}}
  SR:
    name: liif
    args:
      encoder_spec: {{name: rdn, args: {{no_upsampling: true}}}}
      sdf_head: true
      imnet_spec: {{name: mlp, args: {{out_dim: 1, hidden_list: [32, 32]}}}}
  degrade:
    name: simsiam
    args: {{dim: 32}}
  path: null
"""
    
    with open(config_path, 'w') as f:
        f.write(config_content)
    print(f"é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")

def run_training_test(config_path, test_params, num_steps=1):
    """è¿è¡Œè®­ç»ƒæµ‹è¯•"""
    
    with open(config_path, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    train_loader, _ = datasets.make_data_loaders(config, DDP=False, state='SR')
    print("æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸã€‚")

    model = models.make(config['model'], args={'config': config}).to(device)
    optimizer = utils.make_optimizer(model.parameters(), config['optimizer'])
    print("æ¨¡å‹å’Œä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸã€‚")
    
    sr_criterion = nn.L1Loss()
    geom_criterion = nn.MSELoss()
    degrade_op = SRMDPreprocessing()
    pool = dequeue_and_enqueue(config, 'SR').to(device)
    
    print(f"\nå¼€å§‹è®­ç»ƒæµ‹è¯•ï¼Œè¿è¡Œ {num_steps} ä¸ªæ­¥éª¤...")
    
    for step in range(num_steps):
        print(f"\n=== æ­¥éª¤ {step + 1}/{num_steps} ===")
        start_time = time.time()
        
        batch = next(iter(train_loader))
        print("æˆåŠŸè·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ã€‚")
        
        for k, v in batch.items():
            if v is not None:
                batch[k] = v.to(device)
                
        lr = degrade_op(batch['inp'], scale=test_params['scale'], norm=True)
        p = {'lr': lr, 'gt': batch['gt'], 'cell': batch['cell'], 'coord': batch['coord'], 
             'scale': batch['scale'].type(torch.FloatTensor), 'gt_sdf': batch.get('gt_sdf')}
        lr, gt, cell, coord, scale, gt_sdf = pool(p)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        model.train()
        pred_rgb = model(lr, coord, cell, state='train')
        pred_sdf = model.SR.sdf_pred
        
        # è®¡ç®—æŸå¤±
        loss_sr = sr_criterion(pred_rgb, gt)
        total_loss = loss_sr
        loss_geom_val = 0.0
        
        if pred_sdf is not None and gt_sdf is not None:
            # æ–¹æ¡ˆA: ç›´æ¥åœ¨å½’ä¸€åŒ–ç©ºé—´è®¡ç®—SDFæŸå¤±ï¼Œé¿å…åå½’ä¸€åŒ–çš„æ•°å€¼é—®é¢˜
            # è¿™æ ·å¯ä»¥é¿å…å¤§æ•°å€¼å¯¼è‡´çš„æ¢¯åº¦çˆ†ç‚¸é—®é¢˜
            loss_geom = geom_criterion(pred_sdf, gt_sdf)
            total_loss = total_loss + config['lambda_geom'] * loss_geom
            loss_geom_val = loss_geom.item()
            
            # å¦‚æœéœ€è¦ç›‘æ§åŸå§‹ç©ºé—´çš„æŸå¤±ï¼Œå¯ä»¥æ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆä»…ç”¨äºæ‰“å°ï¼Œä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼‰
            # with torch.no_grad():
            #     t = config['sdf_norm']
            #     sdf_sub = t['sub'][0]
            #     sdf_div = t['div'][0]
            #     pred_sdf_orig = pred_sdf * sdf_div + sdf_sub
            #     gt_sdf_orig = gt_sdf * sdf_div + sdf_sub
            #     loss_geom_orig = geom_criterion(pred_sdf_orig, gt_sdf_orig)
            #     print(f"    - åŸå§‹ç©ºé—´SDFæŸå¤±: {loss_geom_orig.item():.4f}")

        # åå‘ä¼ æ’­
        total_loss.backward()
        optimizer.step()
        
        step_time = time.time() - start_time
        print(f"  - SR Loss: {loss_sr.item():.4f}")
        print(f"  - Geom Loss: {loss_geom_val:.4f}")
        print(f"  - Total Loss: {total_loss.item():.4f}")
        print(f"  - æ­¥éª¤ç”¨æ—¶: {step_time:.2f}ç§’")

def validate_data_consistency(liver_data_path, sdf_data_path):
    """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
    print("\n=== æ•°æ®ä¸€è‡´æ€§éªŒè¯ ===")
    
    # æ£€æŸ¥liver_dataä¸­çš„å›¾ç‰‡æ•°é‡
    liver_files = [f for f in os.listdir(liver_data_path) if f.endswith('.png')]
    print(f"Liveræ•°æ®æ–‡ä»¶æ•°é‡: {len(liver_files)}")
    
    # æ£€æŸ¥SDFæ•°æ®
    sdf_data = np.load(sdf_data_path)
    print(f"SDFæ•°æ®å½¢çŠ¶: {sdf_data.shape}")
    print(f"SDFæ•°æ®ç±»å‹: {sdf_data.dtype}")
    print(f"SDFå€¼èŒƒå›´: [{sdf_data.min():.4f}, {sdf_data.max():.4f}]")
    
    # æ£€æŸ¥ä¸€å¼ ç¤ºä¾‹å›¾ç‰‡
    if liver_files:
        sample_img_path = os.path.join(liver_data_path, liver_files[0])
        sample_img = io.imread(sample_img_path)
        print(f"ç¤ºä¾‹å›¾ç‰‡å½¢çŠ¶: {sample_img.shape}")
        print(f"ç¤ºä¾‹å›¾ç‰‡æ•°æ®ç±»å‹: {sample_img.dtype}")
        print(f"ç¤ºä¾‹å›¾ç‰‡å€¼èŒƒå›´: [{sample_img.min()}, {sample_img.max()}]")
    
    # éªŒè¯å½¢çŠ¶åŒ¹é…
    expected_shape = (len(liver_files), sample_img.shape[0], sample_img.shape[1])
    if sdf_data.shape == expected_shape:
        print(f"âœ… æ•°æ®å½¢çŠ¶åŒ¹é…: SDF {sdf_data.shape} == æœŸæœ› {expected_shape}")
    else:
        print(f"âŒ æ•°æ®å½¢çŠ¶ä¸åŒ¹é…: SDF {sdf_data.shape} != æœŸæœ› {expected_shape}")
        return False
    
    return True

def main():
    parser = argparse.ArgumentParser(description="åœ¨çœŸå®liveræ•°æ®ä¸Šæµ‹è¯•è®­ç»ƒæµæ°´çº¿")
    parser.add_argument('--liver_data', default='liver_data', help='Liveræ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--sdf_data', default='sdf_grid_fixed.npy', help='SDFæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config', default='configs/test_real_training.yaml', help='è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--steps', type=int, default=3, help='è¿è¡Œçš„è®­ç»ƒæ­¥éª¤æ•°')
    parser.add_argument('--batch_size', type=int, default=2, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--inp_size', type=int, default=48, help='è¾“å…¥å°ºå¯¸')
    parser.add_argument('--sample_q', type=int, default=2304, help='é‡‡æ ·ç‚¹æ•°é‡')
    parser.add_argument('--scale', type=int, default=8, help='ç¼©æ”¾å› å­')
    parser.add_argument('--lambda_geom', type=float, default=0.1, help='å‡ ä½•æŸå¤±æƒé‡')
    parser.add_argument('--keep_config', action='store_true', help='ä¿ç•™ç”Ÿæˆçš„é…ç½®æ–‡ä»¶')
    
    args = parser.parse_args()
    
    test_params = {
        'batch_size': args.batch_size,
        'inp_size': args.inp_size,
        'sample_q': args.sample_q,
        'scale': args.scale,
        'lambda_geom': args.lambda_geom
    }
    
    try:
        print("ğŸš€ å¼€å§‹çœŸå®æ•°æ®è®­ç»ƒæµæ°´çº¿æµ‹è¯•")
        print(f"å‚æ•°é…ç½®: {test_params}")
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        if not validate_data_consistency(args.liver_data, args.sdf_data):
            print("âŒ æ•°æ®éªŒè¯å¤±è´¥")
            return
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        print(f"\n=== åˆ›å»ºé…ç½®æ–‡ä»¶ ===")
        create_config_for_real_data(args.config, args.liver_data, args.sdf_data, test_params)
        
        # è¿è¡Œè®­ç»ƒæµ‹è¯•
        print(f"\n=== å¼€å§‹è®­ç»ƒæµ‹è¯• ===")
        run_training_test(args.config, test_params, args.steps)
        
        print(f"\nâœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼è¿è¡Œäº† {args.steps} ä¸ªè®­ç»ƒæ­¥éª¤ã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼Œå‡ºç°é”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        # æ¸…ç†é…ç½®æ–‡ä»¶ï¼ˆé™¤éç”¨æˆ·é€‰æ‹©ä¿ç•™ï¼‰
        if not args.keep_config and os.path.exists(args.config):
            try:
                os.remove(args.config)
                print(f"å·²åˆ é™¤ä¸´æ—¶é…ç½®æ–‡ä»¶: {args.config}")
            except OSError as e:
                print(f"åˆ é™¤é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")

if __name__ == '__main__':
    main()
